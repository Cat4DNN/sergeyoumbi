+++
title = "Introduction to Category Theory for AI Practitioners"
description = "A gentle introduction to category theory and why it matters for understanding deep learning architectures and AI systems."
date = 2025-01-15T09:00:00+00:00
draft = false
template = "blog/page.html"

[taxonomies]
tags = ["category-theory", "deep-learning", "tutorial"]

[extra]
lead = "Category theory provides a powerful lens for understanding the compositional nature of neural networks. Let's explore the basics."
math = true
+++

Category theory has been called "the mathematics of mathematics" — a framework so abstract that it can describe relationships across vastly different domains. But what does this have to do with deep learning?

## Why Category Theory for AI?

At its core, deep learning is about **composition**. We compose layers to form networks, compose networks to form systems, and compose transformations to process data. Category theory gives us a precise language for talking about composition.

### The Basic Objects

A **category** $\mathcal{C}$ consists of:

1. A collection of **objects** (think: types, spaces, or data representations)
2. **Morphisms** (arrows) between objects (think: functions, transformations, or neural network layers)
3. A **composition** operation: if $f: A \to B$ and $g: B \to C$, then $g \circ f: A \to C$

$$f: A \to B, \quad g: B \to C \quad \Rightarrow \quad g \circ f: A \to C$$

### The Para Construction

A powerful categorical framework for understanding neural networks is the **Para construction** (Fong, Spivak and Tuyéras, 2019). Given a symmetric monoidal category $(\mathcal{C}, \otimes, I)$, the Para construction creates a new category where:

- **Objects** remain the same as in $\mathcal{C}$
- **Morphisms** from $A$ to $B$ are pairs $(P, f)$ where $P$ is a "parameter object" and $f: P \otimes A \to B$

This elegantly captures how neural network layers have both:
1. **Parameters** (weights and biases) — the object $P$
2. **Computation** (the forward pass) — the morphism $f$

The composition of Para morphisms naturally models how parameters flow through layered architectures. As shown by Cruttwell et al. (2022), this framework extends to capture backpropagation through the notion of **reverse derivative categories**.

$$\text{Para}(\mathcal{C})(A, B) = \coprod_{P \in \mathcal{C}} \mathcal{C}(P \otimes A, B)$$

## Functors: The Key to Transfer Learning

A **functor** $F: \mathcal{C} \to \mathcal{D}$ maps:
- Objects in $\mathcal{C}$ to objects in $\mathcal{D}$
- Morphisms in $\mathcal{C}$ to morphisms in $\mathcal{D}$

While preserving composition: $F(g \circ f) = F(g) \circ F(f)$

This is exactly what happens in transfer learning! A pretrained network on ImageNet (category $\mathcal{C}$) is mapped to a new domain (category $\mathcal{D}$) while preserving the compositional structure.

## Natural Transformations: Attention Mechanisms

When we have two functors $F, G: \mathcal{C} \to \mathcal{D}$, a **natural transformation** $\eta: F \Rightarrow G$ provides a systematic way to transform one into the other.

$$\eta_A: F(A) \to G(A) \quad \text{for all objects } A$$

Attention mechanisms can be viewed through this lens — they provide learned transformations between different representations (queries, keys, values) while respecting the underlying structure.

## Looking Ahead

This is just the beginning. In future posts, we'll explore:

- **Monads** and their role in handling effects in neural networks
- **Adjunctions** and their connection to encoder-decoder architectures
- **String diagrams** for visualising neural network architectures

Category theory isn't just abstract nonsense — it's a powerful tool for understanding, designing, and reasoning about AI systems.

---

*Have questions? Reach out on [Twitter](https://twitter.com/CatDNN) or [contact me](/contact/) directly.*

---

## References

Cruttwell, G.S.H., Gavranović, B., Ghani, N., Wilson, P. and Zanasi, F. (2022) 'Categorical foundations of gradient-based learning', *Programming Languages and Systems: 31st European Symposium on Programming, ESOP 2022*. Cham: Springer, pp. 1-28.

Fong, B., Spivak, D. and Tuyéras, R. (2019) 'Backprop as functor: A compositional perspective on supervised learning', *Proceedings of the 34th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)*. IEEE, pp. 1-13.

Mac Lane, S. (1998) *Categories for the Working Mathematician*. 2nd edn. New York: Springer-Verlag.

Spivak, D.I. (2014) *Category Theory for the Sciences*. Cambridge, MA: MIT Press.
